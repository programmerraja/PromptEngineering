# Prompt Engineering From First Principles: The Mechanics They Don't Teach You

Welcome to my deep dive series on Prompt Engineering.

Most resources out there give you templates or "magic words." This series is different. We are going back to first principles to understand **how Large Language Models (LLMs) actually work**, why they fail, and how to rigorously engineer prompts rather than just guessing.

## The Series

This series is broken down into four parts:

- **[Part 1: The Foundation - How LLMs Really Work](./blog/part-1.md)**  
  _Understanding tokenization, embeddings, and the probability engine behind the curtain._

- **Part 2: The Art & Science of Prompting (planned)**  
  _How word choice, structure, and model personality shape the outputs. The butterfly effect of prompting._

- **Part 3: Prompting techniques and optimization (planned)**
  _What are the prompting techniques and how to optimize the prompt etc_

- **Part 4: Prompt Evaluation and Scaling (Planned)**  
  _Moving from "it looks good to me" to data-driven evaluation and reliable scaling._

- **Part 5: Tips, Tricks, and Experience (Planned)**  
  _Practical lessons from the trenches and advanced techniques._

## About This Repo

This repository contains the source markdown files for the blog series.

- `blog/`: Contains the actual posts.

## Contributing / Feedback

If you spot an error in the technical explanations or have a suggestion for the upcoming parts, feel free to open an issue or PR.

---

_Author: Boopathi K_
